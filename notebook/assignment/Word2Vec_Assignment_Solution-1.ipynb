{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/am88tech/gen-ai-ml/blob/main/notebook/assignment/Word2Vec_Assignment_Solution-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk"
      ],
      "metadata": {
        "id": "qfaTfG5u6spT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load data from google drive https://drive.google.com/file/d/1vZ4S0dtiUk5LqeeccqWs9IAAG8qH1GWv/view?usp=sharing\n",
        "\n",
        "!gdown --id 1vZ4S0dtiUk5LqeeccqWs9IAAG8qH1GWv\n",
        "!unzip News_Category_Dataset.zip"
      ],
      "metadata": {
        "id": "XOFcgfgl8uhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the Dataset\n",
        "data = pd.read_json('/content/News_Category_Dataset_v3.json', lines=True)\n",
        "data\n",
        "\n",
        "# We'll use the 'headline' as the text data and 'category' as the label\n",
        "data = data[['headline', 'category']].dropna()\n",
        "\n",
        "data['processed_text'] = data['headline'].astype(str)\n",
        "print(data['category'].value_counts())\n",
        "\n",
        "#Consider top4 categories only\n",
        "top_categories = ['POLITICS', 'ENTERTAINMENT', 'BUSINESS', 'SPORTS']\n",
        "data = data[data['category'].isin(top_categories)]\n",
        "print(data[\"headline\"].head())\n",
        "print(data['category'].value_counts())"
      ],
      "metadata": {
        "id": "fPsaEbs_6wey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 3: Prepare the Data for TensorFlow\n",
        "\n",
        "# Initialize the Tokenizer to convert text into sequences of integers.\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the 'processed_text' column to learn the vocabulary.\n",
        "tokenizer.fit_on_texts(data['processed_text'])\n",
        "\n",
        "# Convert the text in 'processed_text' column to sequences of integers.\n",
        "sequences = tokenizer.texts_to_sequences(data['processed_text'])\n",
        "\n",
        "# Define the maximum length for the sequences. Any sequences longer than this will be truncated,\n",
        "# and any sequences shorter will be padded.\n",
        "max_length = 100  # Maximum length of a complaint narrative\n",
        "\n",
        "# Pad or truncate the sequences so that they all have the same length of max_length.\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# Convert the 'category' column to numerical values using LabelEncoder.\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['category'])\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "9hHuNoRHCJdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4: Configure the model\n",
        "\n",
        "# Calculate the vocabulary size. The vocabulary size is the total number of unique words in the text data plus one.\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
        "print(vocab_size)\n",
        "\n",
        "# Initialize a Sequential model.\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer. This layer will learn word embeddings for the input sequences.\n",
        "# - input_dim: Size of the vocabulary.\n",
        "# - output_dim: Dimension of the dense embedding.\n",
        "# - input_length: Length of input sequences.\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=32))\n",
        "# Add a GlobalAveragePooling1D layer. This layer calculates the average of all the embeddings in a sequence.\n",
        "# This reduces the dimensionality and helps to prevent overfitting.\n",
        "model.add(GlobalAveragePooling1D())\n",
        "\n",
        "# Add a Dense layer with 64 units and ReLU activation. This layer acts as a hidden layer in the neural network.\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Add a Dense output layer with a number of units equal to the number of unique labels.\n",
        "# The softmax activation function is used to output a probability distribution over the classes.\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "b8Fs416BCYsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 5: Train the Model\n",
        "\n",
        "# Compile the model. The compile step specifies the optimizer, loss function, and evaluation metrics.\n",
        "# - optimizer: 'adam' is a popular optimizer that adjusts the learning rate during training.\n",
        "# - loss: 'sparse_categorical_crossentropy' is used for multi-class classification when labels are provided as integers.\n",
        "# - metrics: 'accuracy' will track the accuracy of the model during training and evaluation.\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the training data.\n",
        "# - X_train: Training input data.\n",
        "# - y_train: Training target data.\n",
        "# - epochs: Number of times to iterate over the training data.\n",
        "# - batch_size: Number of samples per gradient update.\n",
        "# - validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the trained model's weights to a file.\n",
        "model.save_weights('complaints_model.weights.h5')\n",
        "\n",
        "# Load the model weights from the saved file. This step can be used to reload the model for further evaluation or inference.\n",
        "model.load_weights('complaints_model.weights.h5')"
      ],
      "metadata": {
        "id": "LMDZ12JJCyhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 6: Evaluate the Model\n",
        "\n",
        "# Predict the classes for the test data.\n",
        "# The model.predict method returns probabilities for each class, and np.argmax is used to get the class with the highest probability.\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# Calculate the confusion matrix to evaluate the accuracy of the classification.\n",
        "# The confusion matrix shows the number of correct and incorrect predictions for each class.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Calculate the accuracy score by comparing the true labels with the predicted labels.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Print the classification report, which includes precision, recall, f1-score, and support for each class.\n",
        "print(\"Classification Report:\")\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "OvNhc8ZcC8Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a prediction on new narrations\n",
        "\n",
        "# Define a list of new complaint texts to predict their categories.\n",
        "news_new = [\n",
        "    \"\"\"\n",
        "    LOS ANGELES -- With the bases loaded and two outs against one of baseball’s\n",
        "    nastiest relievers, MJ Melendez fought off pitch after pitch … after pitch after pitch … to\n",
        "    keep the at-bat alive in hopes of coming through in the Royals’\n",
        "    best scoring opportunity on Saturday night.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Biden campaign rakes in $28 million for star-studded Los Angeles fundraiser\n",
        "    The massive haul was announced just hours before President Joe Biden appeared\n",
        "    alongside former President Barack Obama, George Clooney and others.\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# Convert the new complaint texts into sequences of integers using the previously fitted tokenizer.\n",
        "new_sequences = tokenizer.texts_to_sequences(news_new)\n",
        "\n",
        "# Pad the sequences so that they all have the same length as the training data (max_length).\n",
        "new_X = pad_sequences(new_sequences, maxlen=max_length)\n",
        "\n",
        "# Predict the class probabilities for the new complaint sequences.\n",
        "new_predictions = model.predict(new_X)\n",
        "\n",
        "# Determine the predicted class for each new complaint by finding the index of the maximum probability.\n",
        "pred_class = np.argmax(new_predictions, axis=1)\n",
        "\n",
        "# Print the predicted class indices.\n",
        "print(pred_class)\n",
        "\n",
        "# Convert the predicted class indices back to class labels using the label encoder.\n",
        "pred_labels = label_encoder.inverse_transform(pred_class)\n",
        "\n",
        "# Print the predicted class labels.\n",
        "print(pred_labels)"
      ],
      "metadata": {
        "id": "MKZNQP6-DGT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BqKjW_VnEOV1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}