{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/am88tech/gen-ai-ml/blob/main/notebook/assignment/Word2Vec_Assignment_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-1 : Import the data\n",
        "https://drive.google.com/file/d/1vZ4S0dtiUk5LqeeccqWs9IAAG8qH1GWv/view?usp=sharing\n",
        "\n",
        "it will be a zipped file, unzip News_Category_Dataset.zip\n"
      ],
      "metadata": {
        "id": "SbXCytD9GRdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Download the Dataset**:\n",
        "   - Use the `gdown` command with the `--id` parameter to download the dataset from Google Drive. Replace `1vZ4S0dtiUk5LqeeccqWs9IAAG8qH1GWv` with the specific ID of your file on Google Drive to ensure the correct file is downloaded.\n",
        "\n",
        "2. **Unzip the Dataset**:\n",
        "   - After downloading, unzip the dataset using the `!unzip` command followed by the filename. In this case, the file to unzip is `News_Category_Dataset.zip`. This step will extract the JSON file needed for data loading.\n",
        "\n",
        "3. **Load the Dataset**:\n",
        "   - Import the `pandas` library (if not already imported) using `import pandas as pd` to handle the dataset. Load the JSON file using `pd.read_json`, specifying the path `/content/News_Category_Dataset_v3.json` and setting `lines=True` to correctly format the dataset as each JSON object is stored on a separate line.\n",
        "\n",
        "4. **Select Relevant Columns**:\n",
        "   - From the loaded dataset, select only the columns 'headline' and 'category' for further analysis. Ensure that any missing values in these columns are removed by using the `.dropna()` method. This will help in maintaining the quality and consistency of the data being analyzed.\n",
        "\n",
        "5. **Preprocess Text Data**:\n",
        "   - Convert the text in the 'headline' column to string type to standardize the format for textual analysis. This is achieved by applying `.astype(str)` to the 'headline' column, which ensures that all entries are treated as strings.\n",
        "\n",
        "6. **Filter and Display Selected Categories**:\n",
        "   - Define a list of categories of interest (e.g., 'POLITICS', 'ENTERTAINMENT', 'BUSINESS', 'SPORTS'). Filter the dataset to include only these categories by checking if the 'category' column values are in the predefined list `top_categories`.\n",
        "   - Display the first few headlines and the count of entries per category in the filtered dataset to verify the filtering process and to get a preliminary view of the data distribution among these top categories. Use `print(data[\"headline\"].head())` and `print(data['category'].value_counts())`.\n",
        "\n"
      ],
      "metadata": {
        "id": "KxFMPJuYICG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!gdown --id 1vZ4S0dtiUk5LqeeccqWs9IAAG8qH1GWv\n",
        "!unzip News_Category_Dataset.zip\n"
      ],
      "metadata": {
        "id": "GO5IZxhSJuFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f89c4b4e-3dc0-43f6-de2d-d8bfc0ffdd85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vZ4S0dtiUk5LqeeccqWs9IAAG8qH1GWv\n",
            "From (redirected): https://drive.google.com/uc?id=1vZ4S0dtiUk5LqeeccqWs9IAAG8qH1GWv&confirm=t&uuid=15b877e4-c07e-4d4e-b97b-17de6678364e\n",
            "To: /content/News_Category_Dataset.zip\n",
            "100% 27.8M/27.8M [00:00<00:00, 69.2MB/s]\n",
            "Archive:  News_Category_Dataset.zip\n",
            "replace News_Category_Dataset_v3.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_json(\"/content/News_Category_Dataset_v3.json\", lines=True)\n",
        "# Select relevant columns and remove missing values\n",
        "data = data[['headline', 'category']].dropna()\n",
        "\n",
        "# Convert 'headline' column to string type\n",
        "data['headline'] = data['headline'].astype(str)\n",
        "\n",
        "# Define top categories\n",
        "top_categories = ['POLITICS', 'ENTERTAINMENT', 'BUSINESS', 'SPORTS']\n",
        "\n",
        "# Filter data based on top categories\n",
        "data = data[data['category'].isin(top_categories)]\n",
        "\n",
        "# Display first few headlines and category counts\n",
        "print(data[\"headline\"].head())\n",
        "print(data['category'].value_counts())"
      ],
      "metadata": {
        "id": "YNwXBlEA96yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Final Expected Output\n",
        "\n",
        "category\n",
        "POLITICS         35602\n",
        "ENTERTAINMENT    17362\n",
        "BUSINESS          5992\n",
        "SPORTS            5077\n",
        "'''"
      ],
      "metadata": {
        "id": "fPsaEbs_6wey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3LOkmYfJxJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step-2: Pre-process, Tokenize and Prepare Train and Test data"
      ],
      "metadata": {
        "id": "sRd9dphJIxme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Initialize the Tokenizer**:\n",
        "   - Start by creating an instance of the `Tokenizer` from the TensorFlow library. This tokenizer will be used to convert text data into sequences of integers, which are more manageable for model processing.\n",
        "\n",
        "2. **Fit the Tokenizer**:\n",
        "   - Fit the tokenizer on the 'processed_text' column of your dataset. This step allows the tokenizer to learn and map the vocabulary of your texts, essential for transforming text into a numerical format.\n",
        "\n",
        "3. **Convert Text to Sequences**:\n",
        "   - Use the tokenizer to convert the texts in the 'processed_text' column into sequences. Each text will be transformed into a sequence of integers where each integer represents a unique word in the learned vocabulary.\n",
        "\n",
        "4. **Set Sequence Length**:\n",
        "   - Define a maximum sequence length (100 in this case) to standardize the size of the input data. This helps in handling variability in text length across your dataset.\n",
        "\n",
        "5. **Pad Sequences**:\n",
        "   - Adjust the sequences to a consistent length using `pad_sequences`. This function will truncate sequences longer than the maximum length and pad shorter ones with zeros. The result is a uniform input shape for modeling, stored in variable `X`.\n",
        "\n",
        "6. **Encode Labels and Split Data**:\n",
        "   - Convert categorical labels in the 'category' column into numerical form using `LabelEncoder`, making them suitable for training a TensorFlow model. Then, split the dataset into training and testing sets with the `train_test_split` method, using 20% of the data for testing, ensuring that your model is trained and evaluated on different subsets of data."
      ],
      "metadata": {
        "id": "d9EdLGGwJLx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your code here\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['headline'])\n",
        "sequences = tokenizer.texts_to_sequences(data['headline'])\n",
        "\n",
        "max_length = 100  # Maximum length of a complaint narrative\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = data['headline']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "G0SX-9I1J_ed"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Final Expected Output\n",
        "\n",
        "Shape of X_train: (51226, 100)\n",
        "Shape of y_train: (51226,)\n",
        "Shape of X_test: (12807, 100)\n",
        "Shape of y_test: (12807,)\n",
        "'''"
      ],
      "metadata": {
        "id": "9hHuNoRHCJdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ApjKTcUKKB8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Configure the model"
      ],
      "metadata": {
        "id": "IQJi_IJqKQr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Calculate Vocabulary Size**:\n",
        "   - Determine the size of the vocabulary by counting the total number of unique words in the text data, which is obtained from the `tokenizer.word_index`. Add one to this count to account for the zero index in TensorFlow.\n",
        "\n",
        "2. **Initialize the Sequential Model**:\n",
        "   - Create an instance of the `Sequential` model from TensorFlow's Keras API. This sets up a linear stack of layers in the neural network, to which you will add different types of layers.\n",
        "\n",
        "3. **Add Embedding Layer**:\n",
        "   - Insert an `Embedding` layer first in the model to convert integer sequences (tokens) into dense vectors of fixed size. Set the `input_dim` to the vocabulary size, `output_dim` to 16/32/64 to define the vector space dimensionality, and `input_length` to the maximum length of input sequences.\n",
        "\n",
        "4. **Incorporate GlobalAveragePooling1D Layer**:\n",
        "   - Include a `GlobalAveragePooling1D` layer following the embedding layer. This layer reduces the dimensionality of the model by calculating the average output of each dimension across the sequence, which helps in minimizing overfitting.\n",
        "\n",
        "5. **Add Dense Hidden Layer**:\n",
        "   - Add a `Dense` layer with 32/64/128 neurons, using the 'ReLU' activation function. This layer serves as the hidden layer and provides the model with the ability to learn non-linear relationships in the data.\n",
        "\n",
        "6. **Configure Output Layer**:\n",
        "   - Finally, add another `Dense` layer, this time with the number of units equal to the number of unique labels in your dataset, using 'softmax' activation. This layer will output a probability distribution over the class labels, making it suitable for multi-class classification. End by printing the model summary to review the architecture and parameters of your network.\n",
        "\n"
      ],
      "metadata": {
        "id": "7egpp9d_KV6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your code here\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))"
      ],
      "metadata": {
        "id": "zZ0ygf44LS72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4c5ae0c1-7502-458d-c943-7dd430177335"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d033f19b6a3f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Vocabulary size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Final Expected Output\n",
        "\n",
        "Model: \"sequential_2\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #\n",
        "=================================================================\n",
        " embedding_2 (Embedding)     (None, 100, 32)           1338496\n",
        "\n",
        " global_average_pooling1d_1  (None, 32)                0\n",
        "  (GlobalAveragePooling1D)\n",
        "\n",
        " dense_2 (Dense)             (None, 64)                2112\n",
        "\n",
        " dense_3 (Dense)             (None, 4)                 260\n",
        "\n",
        "=================================================================\n",
        "Total params: 1340868 (5.12 MB)\n",
        "Trainable params: 1340868 (5.12 MB)\n",
        "Non-trainable params: 0 (0.00 Byte)\n",
        "_________________________________________________________________\n",
        "'''"
      ],
      "metadata": {
        "id": "b8Fs416BCYsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4- Compile and Train the Model"
      ],
      "metadata": {
        "id": "DdLqkjmgLbmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Compile the Model**:\n",
        "   - Set up your model for training by compiling it with the necessary configurations: use 'adam' as the optimizer for its efficiency in handling sparse gradients and adaptive learning rate capabilities; choose 'sparse_categorical_crossentropy' as the loss function suited for multi-class classification tasks where labels are integers; and select 'accuracy' as the metric to monitor the model's performance during training.\n",
        "\n",
        "2. **Configure Training Parameters**:\n",
        "   - Specify the parameters for training the model: `epochs` defines how many times the model will work through the entire training dataset; `batch_size` determines the number of samples to work through before updating the internal model parameters; use `validation_data` to provide the test dataset for evaluating the model after each epoch.\n",
        "\n",
        "3. **Start Model Training**:\n",
        "   - Begin the training process by calling the `model.fit` method with your training dataset (`X_train` and `y_train`), along with the number of epochs, batch size, and validation data. This process iteratively adjusts the model weights to minimize the loss and improve accuracy on the training data.\n",
        "\n",
        "4. **Monitor Training Progress**:\n",
        "   - Observe the output during training to monitor progress. This output includes loss and accuracy metrics for both training and validation sets, providing insight into how well the model is learning and generalizing to new data.\n",
        "\n",
        "5. **Save Model Weights**:\n",
        "   - After training, save the model weights to a file using `model.save_weights('model.h5')`. This allows the trained model configuration to be preserved, which can be useful for deployment or further evaluation without needing to retrain.\n",
        "\n",
        "6. **Reload Model Weights**:\n",
        "   - If needed, reload the model weights from the saved file with `model.load_weights('model.h5')` to resume training, make predictions, or perform evaluations. This step ensures that the model's state can be restored or transferred without loss of fidelity.\n"
      ],
      "metadata": {
        "id": "h36CCM0WLqvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your code here\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "LMDZ12JJCyhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a232690-1f0e-4132-b04f-97e55d0dcf7f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 393ms/step - accuracy: 0.0010 - loss: 11.0792 - val_accuracy: 0.0014 - val_loss: 11.1910\n",
            "Epoch 2/5\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 328ms/step - accuracy: 0.0011 - loss: 11.0015 - val_accuracy: 0.0014 - val_loss: 11.4208\n",
            "Epoch 3/5\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 383ms/step - accuracy: 0.0015 - loss: 10.9364 - val_accuracy: 0.0014 - val_loss: 11.6400\n",
            "Epoch 4/5\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 373ms/step - accuracy: 0.0013 - loss: 10.8881 - val_accuracy: 0.0014 - val_loss: 11.8485\n",
            "Epoch 5/5\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 380ms/step - accuracy: 0.0015 - loss: 10.8535 - val_accuracy: 0.0014 - val_loss: 12.0473\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b46480bd650>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights\n",
        "model.save_weights('complaints_model.weights.h5')\n",
        "\n",
        "# Load the model weights\n",
        "model.load_weights('complaints_model.weights.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "S1nXk8dQwLqF",
        "outputId": "526242e3-dc5b-46a4-d465-52c25b573b8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d39828c827e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complaints_model.weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complaints_model.weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Final Expected Output\n",
        "\n",
        "Epoch 1/5\n",
        "401/401 [==============================] - 9s 20ms/step - loss: 1.0822 - accuracy: 0.5611 - val_loss: 0.9576 - val_accuracy: 0.6851\n",
        "Epoch 2/5\n",
        "401/401 [==============================] - 9s 24ms/step - loss: 0.7040 - accuracy: 0.7567 - val_loss: 0.5884 - val_accuracy: 0.7765\n",
        "Epoch 3/5\n",
        "401/401 [==============================] - 8s 19ms/step - loss: 0.4570 - accuracy: 0.8308 - val_loss: 0.4467 - val_accuracy: 0.8428\n",
        "Epoch 4/5\n",
        "401/401 [==============================] - 10s 26ms/step - loss: 0.3348 - accuracy: 0.8898 - val_loss: 0.4037 - val_accuracy: 0.8565\n",
        "Epoch 5/5\n",
        "401/401 [==============================] - 9s 23ms/step - loss: 0.2668 - accuracy: 0.8870 - val_loss: 0.3747 - val_accuracy: 0.8709\n",
        "'''"
      ],
      "metadata": {
        "id": "h_M9oiW9Ly3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Evaluate the Model"
      ],
      "metadata": {
        "id": "QArrD_KAMwum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Generate Predictions**:\n",
        "   - Use the `model.predict` method on `X_test` to obtain the probabilities for each class. Apply `np.argmax` with `axis=1` to convert these probabilities into actual class predictions, `y_pred`, which indicates the class with the highest probability for each test sample.\n",
        "\n",
        "2. **Import and Compute the Confusion Matrix**:\n",
        "   - Import the `confusion_matrix` function from `sklearn.metrics`. Then, calculate the confusion matrix using the true labels (`y_test`) and the predicted labels (`y_pred`). This matrix will help visualize the accuracy of the predictions across different classes, showing the number of correct and incorrect predictions for each class.\n",
        "\n",
        "3. **Calculate and Display Accuracy**:\n",
        "   - Compute the overall accuracy of the model by comparing `y_test` and `y_pred` using the `accuracy_score` function. Display this value to understand the proportion of correctly predicted instances among the total instances in the test set.\n",
        "\n",
        "4. **Print the Classification Report**:\n",
        "   - Use the `classification_report` function from `sklearn.metrics` to generate a detailed classification report. This report includes metrics such as precision, recall, and F1-score for each class, which are crucial for assessing model performance, especially in multi-class classification tasks.\n",
        "\n",
        "5. **Display Class Names in Reports**:\n",
        "   - Provide the `target_names` parameter with class labels from `label_encoder.classes_` to the `classification_report` function. This makes the report more readable and informative by displaying the actual names of the classes instead of numerical labels.\n",
        "\n",
        "6. **Review Model Performance**:\n",
        "   - Examine the confusion matrix and the classification report printed in the console to review how well the model performs on different classes. Use this analysis to identify any biases or weaknesses in the model, such as consistently misclassified classes, which could guide further refinement and improvements.\n",
        "\n"
      ],
      "metadata": {
        "id": "0JuRb1TTMygX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "#Confusion Matrix\n",
        "cm= tf.math.confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "LoCpPQDpNBZ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "cb77b097-0aff-4392-bff1-1e0396784c61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-990b89880373>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Expected Output\n",
        "\n",
        "Confusion Matrix:\n",
        "[[ 675   85  461   70]\n",
        " [  33 3140  256   41]\n",
        " [ 122  242 6661   15]\n",
        " [  90  161   77  678]]\n",
        "Accuracy: 0.8709299601780276\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "     BUSINESS       0.73      0.52      0.61      1291\n",
        "ENTERTAINMENT       0.87      0.90      0.88      3470\n",
        "     POLITICS       0.89      0.95      0.92      7040\n",
        "       SPORTS       0.84      0.67      0.75      1006\n",
        "\n",
        "     accuracy                           0.87     12807\n",
        "    macro avg       0.83      0.76      0.79     12807\n",
        " weighted avg       0.87      0.87      0.87     12807\n",
        " '''"
      ],
      "metadata": {
        "id": "yP70s4FLNDvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-6 Making a prediction on new news articles"
      ],
      "metadata": {
        "id": "JfXs_liLNOW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_new = [\n",
        "    \"\"\"\n",
        "    LOS ANGELES -- With the bases loaded and two outs against one of baseball’s\n",
        "    nastiest relievers, MJ Melendez fought off pitch after pitch … after pitch after pitch … to\n",
        "    keep the at-bat alive in hopes of coming through in the Royals’\n",
        "    best scoring opportunity on Saturday night.\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Biden campaign rakes in $28 million for star-studded Los Angeles fundraiser\n",
        "    The massive haul was announced just hours before President Joe Biden appeared\n",
        "    alongside former President Barack Obama, George Clooney and others.\n",
        "    \"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "MKZNQP6-DGT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Prepare New Input Data**:\n",
        "   - Create a list of new news articles, `news_new`, where each entry is a string containing the text of the news article. This example includes articles about a baseball game and a political fundraiser.\n",
        "\n",
        "2. **Tokenize New Texts**:\n",
        "   - Use the `tokenizer` that was previously fitted on your training data to convert the new news texts (`news_new`) into sequences of integers. This process transforms the raw text into a format that the neural network can process.\n",
        "\n",
        "3. **Pad Sequences**:\n",
        "   - Pad the newly created sequences (`new_sequences`) to ensure they all have the same length, `max_length`, as defined during the training process. Use the `pad_sequences` function, setting `maxlen` to `max_length`. This standardization is necessary for consistent input size into the neural network.\n",
        "\n",
        "4. **Predict Class Probabilities**:\n",
        "   - Employ the trained model to predict the class probabilities for the padded sequences (`new_X`). The `model.predict` function will output a list of probabilities for each class for each article.\n",
        "\n",
        "5. **Determine Predicted Classes**:\n",
        "   - Extract the predicted class indices by finding the index of the maximum probability in each set of predictions. This is achieved using `np.argmax` across `axis=1` of `new_predictions`, resulting in a list of the most likely class indices for each article.\n",
        "\n",
        "6. **Translate Indices to Labels**:\n",
        "   - Convert the predicted class indices (`pred_class`) back into readable class labels using the `label_encoder`'s `inverse_transform` method. This step maps the numerical indices back to their corresponding categorical labels.\n",
        "   - Finally, print both the predicted class indices and their corresponding labels to see the classification results for the new articles.\n",
        "\n"
      ],
      "metadata": {
        "id": "8xuRTiA1Nrwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sequences = tokenizer.texts_to_sequences(news_new)\n",
        "new_X = pad_sequences(new_sequences, maxlen=max_length)\n",
        "new_predictions = model.predict(new_X)\n",
        "pred_class=np.argmax(new_predictions, axis=1)\n",
        "print(pred_class)"
      ],
      "metadata": {
        "id": "BqKjW_VnEOV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Expected output\n",
        "[3 2]\n",
        "['SPORTS' 'POLITICS']\n",
        "'''"
      ],
      "metadata": {
        "id": "TP2rMZpVN0De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7M7jcv4N3W9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}