{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sSy4juwOkH3E",
        "WEltHv15QZ28",
        "O0P9mjUMlieU"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPLSSG9IjSCAS+zZq2k1JKk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Important Note\n",
        "Utilize applications powered by large language models (LLMs) to search for the necessary code and content to answer all the questions in this assignment. Avoid using Google Search. Create a login ID or account with well-known large language models like Copilot, ChatGPT, Gemini, and Claud. These platforms will provide the resources you need to complete the tasks effectively."
      ],
      "metadata": {
        "id": "VnaoJE-ykDbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1) Use Gemini or any LLM to come up with new business or product ideas for millennials in Bangalore. The ideas should be aimed at software professionals and need only a small amount of money to start. Describe the prompts you used and share 3-5 of the most interesting ideas it came up with."
      ],
      "metadata": {
        "id": "sSy4juwOkH3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install OpenAI\n",
        "!pip install langchain\n",
        "!pip install Cohere\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "ABlXwnGEkYFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import Cohere\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key from userdata\n",
        "cohere_api_key = userdata.get('COHERE')\n",
        "\n",
        "# Set the COHERE_API_KEY environment variable\n",
        "os.environ[\"COHERE_API_KEY\"] = cohere_api_key\n",
        "\n",
        "# Now initialize the Cohere LLM\n",
        "llm = Cohere()\n",
        "prompt = \"\"\"\n",
        "Consider yourself to be a software professional. Suggest some new business or product ideas for millennials in Bangalore which nees small amount of money to start\n",
        "\"\"\"\n",
        "result = llm.invoke(prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "as3s5HO6lHNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3) Building and Validating the model"
      ],
      "metadata": {
        "id": "kLrxsF5ySt1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.1) Write code to import data from the below location\n",
        "\n",
        "https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Credit%20Risk%20Data_balanced/Credit_risk_data_bal_v2.csv"
      ],
      "metadata": {
        "id": "WEltHv15QZ28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write code to import data from the below location\n",
        "# https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Credit%20Risk%20Data_balanced/Credit_risk_data_bal_v2.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Credit%20Risk%20Data_balanced/Credit_risk_data_bal_v2.csv\"\n",
        "df = pd.read_csv(url)\n",
        "# You can now work with the dataframe 'df'\n",
        "# For example, to display the first 5 rows:\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "HiyigOE4bbPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.2)  Write code for Splitting the data into Train Test"
      ],
      "metadata": {
        "id": "c5oHa-O_R7_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "YjiIji2HR8x_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "270ijPOYbc0m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (test.shape)\n",
        "print (train.shape)"
      ],
      "metadata": {
        "id": "sF6JQcaypeJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.3) Write code to build the model. Use 'SeriousDlqin2yrs' as target  variable and rest all as predictor variables. Print the beta co-efficients"
      ],
      "metadata": {
        "id": "0I4HfcVHRkQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write code to build the model. Use 'SeriousDlqin2yrs' as target variable and rest all as predictor variables. Print the beta co-efficients using sklearn\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X_train = train.drop('SeriousDlqin2yrs', axis=1)\n",
        "y_train = train['SeriousDlqin2yrs']\n",
        "X_test = test.drop('SeriousDlqin2yrs', axis=1)\n",
        "y_test = test['SeriousDlqin2yrs']\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "model = LogisticRegression() # Use liblinear solver for small datasets\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the beta coefficients (coefficients of the predictors)\n",
        "print(\"Beta Coefficients:\")\n",
        "for feature, coef in zip(X_train.columns, model.coef_[0]):\n",
        "  print(f\"{feature}: {coef}\")\n",
        "\n",
        "print(\"\\nIntercept:\", model.intercept_[0])\n"
      ],
      "metadata": {
        "id": "VM2HNry-baU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.4)Perform Model Validation and Cross Validation on train and test data. Print the confusion matrix and accuracies"
      ],
      "metadata": {
        "id": "PyzCtKBdRnBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Perform Model Validation and Cross Validation on train and test data. Print the confusion matrix and accuracies\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Predict on the training set\n",
        "y_train_pred = model.predict(X_train)\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_confusion_matrix = confusion_matrix(y_train, y_train_pred)\n",
        "\n",
        "print(\"Training Set Performance:\")\n",
        "print(f\"Accuracy: {train_accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{train_confusion_matrix}\")\n",
        "\n",
        "# Perform cross-validation on the training set (e.g., 5-fold)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(f\"\\nCross-Validation Scores (Training): {cv_scores}\")\n",
        "print(f\"Mean CV Accuracy: {cv_scores.mean()}\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nTest Set Performance:\")\n",
        "print(f\"Accuracy: {test_accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{test_confusion_matrix}\")\n"
      ],
      "metadata": {
        "id": "TFT1nMs7bd6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mTmMY4KbeBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4) Download the image from the below location and answer the below questions\n",
        "https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Inoices/Invoice_3.png\n",
        "\n"
      ],
      "metadata": {
        "id": "55Wl2GxFVAhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2) Use Claude or any LLM to make 5 multiple-choice questions on pandas data handling and 5 on numpy. Try generating the questions several times and choose the best ones. Review the questions to ensure they are good and correct."
      ],
      "metadata": {
        "id": "O0P9mjUMlieU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\n",
        "\"\"\"\n",
        "Consider yourself an examiner. Create 5 multiple choice questions on data handling by pandas and 5 multiple choice questions on data handling by numpy. Choose the best questions after\n",
        "\"\"\"\n",
        "result = llm.invoke(prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "RB6D-UxlllAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}