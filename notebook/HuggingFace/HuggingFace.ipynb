{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/am88tech/gen-ai-ml/blob/main/notebook/HuggingFace_v7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDs9DRHeU0lA"
   },
   "source": [
    "##Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UkbG0gAAOzf"
   },
   "outputs": [],
   "source": [
    "!pip -q install accelerate -U\n",
    "!pip install -q 'transformers[torch]'\n",
    "!pip -q install datasets\n",
    "!pip install matplotlib\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install plotly\n",
    "#Restart after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fo9M10uMU5Ea"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDK0x4ifHzdo"
   },
   "outputs": [],
   "source": [
    "# Textwrp function to display the output in a better format\n",
    "# This is an optional function, you can ignore it\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def wrap_display():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', wrap_display)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUKTA2TCK0Bm"
   },
   "source": [
    "# Hugging Face models - Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owPpa22P79T4"
   },
   "source": [
    "## Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "Iqbx0s86EKjR",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "hasDCZe3EKr5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model = pipeline(task=\"sentiment-analysis\")\n",
    "senti_model(\"This movie is damn good. I loved it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLm9YvnHEKx7"
   },
   "outputs": [],
   "source": [
    "senti_model(\"This is a bad phone. The screen and battery are of poor quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUr-TS8_8AMH"
   },
   "source": [
    "## Sentiment Analysis Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hno4ArGtACMZ"
   },
   "outputs": [],
   "source": [
    "Senti_model_2 = pipeline(task=\"sentiment-analysis\",\n",
    "                         model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_f-HrS2AUqW"
   },
   "outputs": [],
   "source": [
    "Senti_model_2(\"Over heating issue don't by this product camera was good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTVT8t5AAusv"
   },
   "outputs": [],
   "source": [
    "Senti_model_2(\"Waste of money\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ajo60hlA34F"
   },
   "outputs": [],
   "source": [
    "Senti_model_2(\"Nice product under 24k .... overall good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnHzKrVbOcB0"
   },
   "source": [
    "## Prediction on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3xBKkh9Ohoy"
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "# Create an unverified SSL context\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# Use urllib to open the URL with the unverified context and then read it with pandas\n",
    "url = \"https://raw.githubusercontent.com/am88tech/gen-ai-ml/refs/heads/main/data/Amazon_Yelp_Reviews/Review_Data.csv\"\n",
    "response = urllib.request.urlopen(url, context=context)\n",
    "\n",
    "# Read the CSV data from the response into a pandas DataFrame\n",
    "user_review_data = pd.read_csv(response)\n",
    "\n",
    "# Sample 50 reviews\n",
    "user_review_data = user_review_data.sample(50)\n",
    "\n",
    "# Display the 'Review' column\n",
    "print(user_review_data[\"Review\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orvgbqqMOhr6"
   },
   "outputs": [],
   "source": [
    "user_review_data[\"Predicted_Sentiment\"] = user_review_data[\"Review\"].apply(lambda x: Senti_model_2(x)[0][\"label\"])\n",
    "user_review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBFCPGdZpCHm"
   },
   "source": [
    "## Load the model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKL7cOZbUvHR"
   },
   "outputs": [],
   "source": [
    "Senti_model_2_gpu = pipeline(task=\"sentiment-analysis\",\n",
    "                         model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLG1kfWYWJYR"
   },
   "outputs": [],
   "source": [
    "user_review_data[\"Predicted_Sentiment\"] = user_review_data[\"Review\"].apply(lambda x: Senti_model_2_gpu(x)[0][\"label\"])\n",
    "user_review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XuiFIE2lP3F"
   },
   "source": [
    "## Language Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOD9eWGClMHd"
   },
   "outputs": [],
   "source": [
    "translator_model = pipeline(task=\"translation_en_to_fr\",\n",
    "                            model=\"google-t5/t5-small\")\n",
    "translator_model(\"Good bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XFeJM7wpfGj"
   },
   "outputs": [],
   "source": [
    "#Clear the cache in GPU\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrFOXzUCtj9D"
   },
   "source": [
    "## Question and Answer Based on a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "NrQG06JItsjz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "qa_model = pipeline(task=\"question-answering\",\n",
    "                    model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsAHxf1xyNZA"
   },
   "outputs": [],
   "source": [
    "# If you get any locale related error\n",
    "'''\n",
    "import locale\n",
    "print(locale.getpreferredencoding())\n",
    "\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PJYJJ2upt2r6"
   },
   "outputs": [],
   "source": [
    "#Importing computer_scientists.txt document from github\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/am88tech/gen-ai-ml/refs/heads/main/data/computer_scientists/computer_scientists.txt\"\n",
    "response = requests.get(url)\n",
    "document = response.text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "y3H2_V_6t60E"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.0078034489415585995,\n",
       " 'start': 518,\n",
       " 'end': 530,\n",
       " 'answer': 'Ada Lovelace'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model({'question':\"Who is the first computer programmer?\",\n",
    "          'context':document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kWnKev1hvk6h"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.24149183928966522,\n",
       " 'start': 330,\n",
       " 'end': 377,\n",
       " 'answer': 'Revolutionized AI for image and text processing'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model({'question':\"What did Yann LeCun contribute?\",\n",
    "          'context':document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vTT84KvAz81F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7583280205726624,\n",
       " 'start': 1421,\n",
       " 'end': 1444,\n",
       " 'answer': 'Geoffrey Everest Hinton'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model({'question':\"Who is the father of deep learning?\",\n",
    "          'context':document})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k91FClC4rUhC"
   },
   "source": [
    "## NER (Name Entity Recognition) Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "UAJDsLvs3ctv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ner_model = pipeline(task=\"ner\",\n",
    "                     model=\"dslim/bert-base-NER\",\n",
    "                     aggregation_strategy=\"simple\")\n",
    "#aggregation_strategy =\"Simple\" ; simplifies the output and makes it easy to read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "VcYMkTmQ3gu2"
   },
   "outputs": [],
   "source": [
    "sample_doc=\"\"\"\n",
    "Hello,\n",
    "  I, John Smith, a member of the Tech Innovators team, would like to schedule a meeting with you,\n",
    "  Mary Johnson, from the Quantum Solutions group, on Tuesday, February 8th, 2024, at 10:00 AM.\n",
    "  We can meet at your office in San Francisco or, if more convenient, at the Cafe Bella in New York City.\n",
    "  Please let me know if this date and time work for you.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Bvn5Vlwv5L7P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9994373, 'word': 'John Smith', 'start': 13, 'end': 23}, {'entity_group': 'ORG', 'score': 0.9968871, 'word': 'Tech Innovators', 'start': 41, 'end': 56}, {'entity_group': 'PER', 'score': 0.99906284, 'word': 'Mary Johnson', 'start': 108, 'end': 120}, {'entity_group': 'ORG', 'score': 0.9988469, 'word': 'Quantum Solutions', 'start': 131, 'end': 148}, {'entity_group': 'LOC', 'score': 0.9993695, 'word': 'San Francisco', 'start': 233, 'end': 246}, {'entity_group': 'ORG', 'score': 0.63540447, 'word': 'Cafe Bella', 'start': 278, 'end': 288}, {'entity_group': 'LOC', 'score': 0.9994677, 'word': 'New York City', 'start': 292, 'end': 305}]\n"
     ]
    }
   ],
   "source": [
    "entities = ner_model(sample_doc)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pgQMV9UQ5rlS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                word entity_group\n",
      "0         John Smith          PER\n",
      "1    Tech Innovators          ORG\n",
      "2       Mary Johnson          PER\n",
      "3  Quantum Solutions          ORG\n",
      "4      San Francisco          LOC\n",
      "5         Cafe Bella          ORG\n",
      "6      New York City          LOC\n"
     ]
    }
   ],
   "source": [
    "# Convert the above output into a dataframe and print it with the entity name\n",
    "NER_result = pd.DataFrame(entities, columns=[\"word\", \"entity_group\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(NER_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lrcBe5ejRV9"
   },
   "source": [
    "## Text Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sA5GowuK6TCR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "summarizer_model = pipeline(task=\"summarization\",\n",
    "                            model=\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4Z-8rQXe6bfF"
   },
   "outputs": [],
   "source": [
    "Book_essay = \"\"\"\n",
    "The 7 Habits of Highly Effective People\" is a timeless self-help book by Stephen R. Covey that offers a holistic approach to personal and professional effectiveness. The book is a guide to transforming one's life by adopting seven fundamental habits.\n",
    "Covey's philosophy centers on the idea that true success is achieved by aligning one's values with principles that govern human effectiveness. The first three habits focus on personal development, emphasizing the importance of taking control of one's life, setting clear goals, and prioritizing tasks based on importance rather than urgency.\n",
    "The next three habits delve into the concept of interdependence, emphasizing the significance of effective communication, cooperation, and collaboration in achieving mutually beneficial outcomes. Covey argues that fostering strong interpersonal relationships and empathetic listening are key to building trust and synergy.\n",
    "The seventh habit, \"Sharpen the Saw,\" encourages continuous self-renewal and personal growth through physical, mental, emotional, and spiritual well-being.\n",
    "Throughout the book, Covey provides practical advice and real-life examples to illustrate each habit's application in various aspects of life, from family and work to leadership and community involvement. \"The 7 Habits of Highly Effective People\" has had a profound impact on individuals seeking personal and professional growth, offering a framework for achieving lasting success and a sense of fulfillment..\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "c_EDurlV7u9Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': '\"The 7 Habits of Highly Effective People\" is a timeless self-help book by Stephen R. Covey that offers a holistic approach to personal and professional effectiveness.'}]\n"
     ]
    }
   ],
   "source": [
    "print(summarizer_model(Book_essay, max_length=120, min_length=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq-viv3QDAT_"
   },
   "source": [
    "## Text Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-9jLaxAj7_Nz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "text_generator_model = pipeline(task=\"text-generation\",\n",
    "                                model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vYPSOfJlDLvo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The best way to start a presentation is to listen to two or three different speakers.\\n\\nOne speaker is typically sitting on either side of the booth by himself or in front of their speakers. In the office there are several desks to do this work'}]\n"
     ]
    }
   ],
   "source": [
    "# Generate text starting with the given prompt\n",
    "text_result = text_generator_model(\"The best way to start a presentation is\")\n",
    "print(text_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlqySLbBv1rD"
   },
   "source": [
    "# Hugging Face models without pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMErYtwhK-CV"
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAtrFzinDMqI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I3a2q9L23ui"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "raw_text = \"This is a great book\"\n",
    "encoded_input = tokenizer(raw_text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits.detach().numpy()\n",
    "y_pred = np.argmax(logits)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jISpOd5j1WNY"
   },
   "outputs": [],
   "source": [
    "#Code for passing multiple examples to the above model\n",
    "\n",
    "import numpy as np\n",
    "# Prepare the input texts\n",
    "texts = [\n",
    "    \"This is a great book\",\n",
    "    \"The food was not tasty and it was very cold\",\n",
    "    \"The weather is very good today\",\n",
    "]\n",
    "\n",
    "# Tokenize and encode the input texts\n",
    "encoded_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the encoded inputs to the model\n",
    "outputs = model(**encoded_inputs)\n",
    "\n",
    "# Get the model's predictions\n",
    "logits = outputs.logits.detach().cpu().numpy()\n",
    "\n",
    "# Find the predicted class for each input\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EbNv3bmBF7V"
   },
   "source": [
    "# Finetuning HuggingFace model\n",
    "Code Explanation- [Click here](https://github.com/venkatareddykonasani/Assorted/blob/main/Fine_tuning_HuggingFace.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYFE5O8RB4MD"
   },
   "source": [
    "### Bank Complaints Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGkqrKxCfrHy"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/venkatareddykonasani/Datasets/raw/master/Bank_Customer_Complaints/complaints_v2.zip\n",
    "!unzip -o complaints_v2.zip\n",
    "complaints_data = pd.read_csv(\"/content/complaints_v2.csv\")\n",
    "complaints_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqmjOcoOB-ec"
   },
   "source": [
    "### Use distilbert model without finetunung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BJkxMer_wjr"
   },
   "outputs": [],
   "source": [
    "# Distil bert model\n",
    "from transformers import pipeline\n",
    "distilbert_model = pipeline(task=\"text-classification\",\n",
    "                            model=\"distilbert-base-uncased\",\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WbHzWMRDiRW"
   },
   "outputs": [],
   "source": [
    "sample_data=complaints_data.sample(100, random_state=42)\n",
    "sample_data[\"text\"]=sample_data[\"text\"].apply(lambda x: \" \".join(x.split()[:350]))\n",
    "sample_data[\"bert_predicted\"] = sample_data[\"text\"].apply(lambda x: distilbert_model(x)[0][\"label\"])\n",
    "#Default prediction is not a number LABEL_1, LABEL_0\n",
    "sample_data[\"bert_predicted_num\"]=sample_data[\"bert_predicted\"].apply(lambda x: x[-1])\n",
    "sample_data[\"bert_predicted_num\"] = sample_data[\"bert_predicted_num\"].astype(int)\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRx_9gXrklH4"
   },
   "source": [
    "### Accuracy of the model without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_LxeuwUIg6r"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(sample_data[\"label\"], sample_data[\"bert_predicted_num\"])\n",
    "print(cm)\n",
    "accuracy=cm.diagonal().sum()/cm.sum()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IoArWg3ksvC"
   },
   "source": [
    "## Project - Finetuning the model with our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvhIoOLQBG8I"
   },
   "outputs": [],
   "source": [
    "!pip -q install accelerate -U\n",
    "!pip -q install transformers[torch]\n",
    "!pip -q install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZT4RxigoO3a"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict, ClassLabel, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF1wh7rY4MtP"
   },
   "outputs": [],
   "source": [
    "#The target variable must be named as \"label\" - Verify it, before proceeding\n",
    "print(sample_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXivZhOxjQtA"
   },
   "outputs": [],
   "source": [
    "Sample_data = Dataset.from_pandas(sample_data)\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = Sample_data.train_test_split(test_size=0.2)  # 80% training, 20% testing\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYxjyEgRR2p-"
   },
   "source": [
    "### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khB_bZv0lcXL"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'} )\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Q_vJOISAtX"
   },
   "source": [
    "### Load and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcBw9CWmf0_D"
   },
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                            num_labels=2,\n",
    "                                                            pad_token_id=tokenizer.eos_token_id) # Adjust num_labels as needed\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbMzSADKvCtB"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_bert_custom\",\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs_bert_custom\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIVONC0H7YZc"
   },
   "outputs": [],
   "source": [
    "# Define the directory where you want to save your model and tokenizer\n",
    "model_dir = \"./distilbert_finetuned\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "#Save the model with\n",
    "trainer.save_model('Distilbert_CustomModel_10K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5zPdvspOrjc"
   },
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "  new_complaint=text\n",
    "  inputs=tokenizer(new_complaint, return_tensors=\"pt\")\n",
    "  inputs = inputs.to(torch.device(\"cuda:0\"))\n",
    "  outputs=model(**inputs)\n",
    "  predictions=outputs.logits.argmax(-1)\n",
    "  predictions=predictions.detach().cpu().numpy()\n",
    "  return(predictions)\n",
    "\n",
    "sample_data[\"finetuned_predicted\"]=sample_data[\"text\"].apply(lambda x: make_prediction(str(x))[0])\n",
    "sample_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8BsIrTyWc72"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Create the confusion matrix\n",
    "cm1 = confusion_matrix(sample_data[\"label\"], sample_data[\"finetuned_predicted\"])\n",
    "print(cm1)\n",
    "accuracy1=cm1.diagonal().sum()/cm1.sum()\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIwrPpOpzawh"
   },
   "source": [
    "### Loading a pre-built model and making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmhTs6AAzFDI"
   },
   "outputs": [],
   "source": [
    "#Code to donwloading the distilbert model\n",
    "!gdown --id 1785J3ir19RaZP3ebbFvWUX88PMaBouro -O distilbert_finetuned_V1.zip\n",
    "!unzip -o -j distilbert_finetuned_V1.zip -d distilbert_finetuned_V1\n",
    "\n",
    "model_v1 = DistilBertForSequenceClassification.from_pretrained('/content/distilbert_finetuned_V1')\n",
    "model_v1.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCJz8FD99xcK"
   },
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "  new_complaint=text\n",
    "  inputs=tokenizer(new_complaint, return_tensors=\"pt\")\n",
    "  inputs = inputs.to(torch.device(\"cuda:0\"))\n",
    "  outputs=model_v1(**inputs)\n",
    "  predictions=outputs.logits.argmax(-1)\n",
    "  predictions=predictions.detach().cpu().numpy()\n",
    "  return(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99Z7s9P-C-hg"
   },
   "outputs": [],
   "source": [
    "sample_data_large=complaints_data.sample(n=1000, random_state=55)\n",
    "sample_data_large[\"finetuned_predicted\"]=sample_data_large[\"text\"].apply(lambda x: make_prediction(str(x)[:350])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLAOpceYfkWI"
   },
   "outputs": [],
   "source": [
    "sample_data_large[\"finetuned_predicted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9Iq2KJD-AJ5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Create the confusion matrix\n",
    "cm1 = confusion_matrix(sample_data_large[\"label\"], sample_data_large[\"finetuned_predicted\"])\n",
    "print(cm1)\n",
    "accuracy1=cm1.diagonal().sum()/cm1.sum()\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vm1l0v6Dx-jb"
   },
   "source": [
    "# Saving the Model on HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6SPRqIiynQW"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install -U ipykernel #for executing the commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcqXDgoizIyD"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gmtZkvqDJNV"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1785J3ir19RaZP3ebbFvWUX88PMaBouro -O distilbert_finetuned_V1.zip\n",
    "!unzip -o -j distilbert_finetuned_V1.zip -d distilbert_finetuned_V1\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('/content/distilbert_finetuned_V1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJmy5o-I0SGn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"YOUR ACCESS TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs-qSjk-z0dd"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#To get Auth token: Profile >> Settings >>Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRPRvUh-0esS"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"venkatareddykonasani/Bank_distil_bert_10K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFnoFMsHBEw8"
   },
   "source": [
    "# Loading the model from HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FbKUERK0k5p"
   },
   "outputs": [],
   "source": [
    "model=DistilBertForSequenceClassification.from_pretrained(\"venkatareddykonasani/Bank_distil_bert_10K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsKSd1EM1DS1"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGd2quFz1Hss"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "!wget https://github.com/venkatareddykonasani/Datasets/raw/master/Bank_Customer_Complaints/complaints_v2.zip\n",
    "!unzip -o complaints_v2.zip\n",
    "complaints_data = pd.read_csv(\"/content/complaints_v2.csv\")\n",
    "list(complaints_data[\"text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyRrPXtF2YFe"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_2szgSb1rcc"
   },
   "outputs": [],
   "source": [
    "complaint=\"\"\"\n",
    "payment history missing credit report made mistake put account forbearance without authorization knowledge matter fact automatic payment setup month monthly mortgage paid full noticed issue account marked forbearance credit report tried get new home loan another new bank contacted immediately asked fix error provide letter detail please see asks forbearance issue seemed fixed however credit report payment history missing new bank able approve new loan issue missing payment history contacted time since phone ask thing report payment history transunion fix missing data issue provide letter show account never forbearance payment history past month however waiting week countless email phone call talk multiple supervisor able get either one thing without issue fixed new bank process new loan application therefore need help immediately get fixed\n",
    "\"\"\"\n",
    "\n",
    "inputs=tokenizer(complaint, return_tensors=\"pt\")\n",
    "outputs=model(**inputs)\n",
    "predictions=outputs.logits.argmax(-1)\n",
    "predictions=predictions.detach().cpu().numpy()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTFBcRMfBM6W"
   },
   "source": [
    "# Web App Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "oIkhfFOwkLY7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "streamlit\n",
    "numpy\n",
    "pandas\n",
    "torch\n",
    "transformers\n",
    "huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Opz4IsR8mOH8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit (from -r requirements.txt (line 1))\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (4.51.3)\n",
      "Requirement already satisfied: huggingface_hub in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.30.2)\n",
      "Collecting altair<6,>=4.0 (from streamlit->-r requirements.txt (line 1))\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit->-r requirements.txt (line 1))\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit->-r requirements.txt (line 1))\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from streamlit->-r requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: packaging<25,>=20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from streamlit->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from streamlit->-r requirements.txt (line 1)) (11.2.1)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from streamlit->-r requirements.txt (line 1)) (4.25.6)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from streamlit->-r requirements.txt (line 1)) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from streamlit->-r requirements.txt (line 1)) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit->-r requirements.txt (line 1))\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit->-r requirements.txt (line 1))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from streamlit->-r requirements.txt (line 1)) (4.12.2)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit->-r requirements.txt (line 1))\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->-r requirements.txt (line 1))\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from streamlit->-r requirements.txt (line 1)) (6.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.18.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (1.35.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1))\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch->-r requirements.txt (line 4)) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1))\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.24.0)\n",
      "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: toml, tenacity, smmap, cachetools, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-5.5.2 gitdb-4.0.12 gitpython-3.1.44 pydeck-0.9.1 smmap-5.0.2 streamlit-1.44.1 tenacity-9.1.2 toml-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6ZYun4Kf2UkY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('venkatareddykonasani/Bank_distil_bert_10K')\n",
    "\n",
    "st.title(\"Bank Complaints Categorization\")\n",
    "st.write(\"Sample Complaints are given below\")\n",
    "Sample_Complaints = [\n",
    "    {\"Sentence\": \"Credit Report - payment history missing credit report made mistake put account forbearance without authorization \"},\n",
    "    {\"Sentence\": \"Retail Related - forwarded message cc sent friday pdt subject final legal payment well fargo well fargo clearly wrong need look actually opened account see court hearing several different government agency \"}\n",
    "]\n",
    "st.table(Sample_Complaints)\n",
    "user_input = st.text_input(\"Enter a complaint:\")\n",
    "button=st.button(\"Classify\")\n",
    "\n",
    "d={\n",
    "    0: \"Credit reporting\",\n",
    "    1: \"Mortgage and Others\"\n",
    "}\n",
    "\n",
    "if user_input and button:\n",
    "  inputs=tokenizer(user_input, return_tensors=\"pt\")\n",
    "  outputs=model(**inputs)\n",
    "  predictions=outputs.logits.argmax(-1)\n",
    "  predictions=predictions.detach().cpu().numpy()\n",
    "  print(predictions)\n",
    "  st.write(\"Prediction :\" , d[predictions[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YIaHczsX-wT0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.219.164.162\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n",
    "\n",
    "#This sometimes doesn't work on Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4zPgyAw_-ko"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
